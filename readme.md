# ğŸ“š Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø¢Ù…ÙˆØ²Ø´ TTS ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ VITS2 Ø±ÙˆÛŒ HuggingFace

## ğŸ“‹ Ù…Ø´Ø®ØµØ§Øª Ù¾Ø±ÙˆÚ˜Ù‡
- **Ø¯ÛŒØªØ§Ø³Øª**: 6795 Ù†Ù…ÙˆÙ†Ù‡ Ã— 15 Ø«Ø§Ù†ÛŒÙ‡ = ~28 Ø³Ø§Ø¹Øª ØµØ¯Ø§ (Ø¹Ø§Ù„ÛŒ!)
- **Ø³Ø±ÙˆØ±**: H200 140GB VRAM + 197GB RAM + 1TB Disk
- **Ù…Ø¯Ù„**: VITS2 Single-Speaker Persian TTS

---

## ğŸš€ **Ù…Ø±Ø­Ù„Ù‡ 1: Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ VM**

```bash
# 1.1 - Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø³ÛŒØ³ØªÙ… Ùˆ Ù†ØµØ¨ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
sudo apt-get update && sudo apt-get upgrade -y
sudo apt-get install -y \
    build-essential \
    cmake \
    wget \
    curl \
    nvtop \
    tmux \
    ffmpeg \
    libsndfile1 \
    espeak-ng \
    libespeak-ng1 \
    python3-pip

sudo apt update
sudo apt install -y \
  build-essential \
  libssl-dev \
  zlib1g-dev \
  libbz2-dev \
  libreadline-dev \
  libsqlite3-dev \
  libncurses5-dev \
  libncursesw5-dev \
  xz-utils \
  tk-dev \
  libffi-dev \
  liblzma-dev \
  uuid-dev \
  wget \
  curl \
  git


# 1.3 - ØªÙ†Ø¸ÛŒÙ… Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# 1.4 - Ø¨Ø±Ø±Ø³ÛŒ GPU
nvidia-smi
```

---

## ğŸ **Ù…Ø±Ø­Ù„Ù‡ 2: Ù†ØµØ¨ Miniconda Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ÛŒØ·**

```bash
# 2.1 - Ú©Ù„ÙˆÙ† repository
git clone https://github.com/p0p4k/vits2_pytorch.git

# 2.2 - Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ÛŒØ· Ù…Ø¬Ø§Ø²ÛŒ
# Ø§ÛŒØ¬Ø§Ø¯ venv
cd ~/vits2_pytorch
python3 -m venv venv

# ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
source venv/bin/activate

# Ø¢Ù¾Ú¯Ø±ÛŒØ¯ pip
pip install --upgrade pip setuptools wheel

# Ø¨Ø±Ø§ÛŒ CUDA 12.8 Ùˆ H200
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Ø¨Ø±Ø±Ø³ÛŒ Ù†ØµØ¨ ØµØ­ÛŒØ­
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0)}')"
```

---

## ğŸ“¦ **Ù…Ø±Ø­Ù„Ù‡ 3: Ù†ØµØ¨ VITS2 Ùˆ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§**

```bash
# 3.2 - Ù†ØµØ¨ requirements
pip install -r requirements.txt

# 3.3 - Ù†ØµØ¨ Ù¾Ú©ÛŒØ¬â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
pip install \
    huggingface_hub \
    datasets \
    soundfile \
    librosa \
    tensorboard \
    wandb \
    matplotlib \
    phonemizer \
    hazm \
    tqdm \
    jiwer \
    torchmetrics

# 3.4 - Build monotonic alignment
cd monotonic_align
mkdir -p monotonic_align
python setup.py build_ext --inplace
cd ..

# 3.5 - ÙˆØ±ÙˆØ¯ Ø¨Ù‡ HuggingFace
huggingface-cli login
# ØªÙˆÚ©Ù† Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯
```

---

## ğŸµ **Ù…Ø±Ø­Ù„Ù‡ 4: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø³Øª**

### 4.1 - Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡

```bash
cat > prepare_dataset.py << 'EOF'
import os
import re
import random
import unicodedata
import soundfile as sf
import librosa
import numpy as np
from datasets import load_dataset, Audio
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor
import warnings
warnings.filterwarnings('ignore')

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
HF_DATASET = "navidved/approved-tts-dataset"
OUT_WAV_DIR = "data_fa/wavs"
OUT_FL_DIR = "filelists"
SR = 22050
MIN_DUR = 1.0   # Ø­Ø¯Ø§Ù‚Ù„ 1 Ø«Ø§Ù†ÛŒÙ‡
MAX_DUR = 15.0  # Ø­Ø¯Ø§Ú©Ø«Ø± 15 Ø«Ø§Ù†ÛŒÙ‡

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒâ€ŒÙ‡Ø§
os.makedirs(OUT_WAV_DIR, exist_ok=True)
os.makedirs(OUT_FL_DIR, exist_ok=True)

# Ù†Ú¯Ø§Ø´Øª Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
ARABIC_TO_FA = {
    '\u064A': 'ÛŒ', '\u0643': 'Ú©', '\u06C0': 'Ù‡', 
    '\u06CC': 'ÛŒ', '\u0649': 'ÛŒ', '\u06A9': 'Ú©'
}

# ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
PERSIAN_DIGITS = "Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹"
LATIN_DIGITS = "0123456789"
DIGIT_MAP = {ord(p): LATIN_DIGITS[i] for i, p in enumerate(PERSIAN_DIGITS)}

# Ø­Ø±Ú©Ø§Øª Ø¹Ø±Ø¨ÛŒ
DIACRITICS = ''.join(chr(c) for c in range(0x064B, 0x065F+1)) + "\u0670"
DIAC_RE = re.compile(f"[{re.escape(DIACRITICS)}]")

def normalize_persian_text(text):
    """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ú©Ù†ØªØ±Ù„ÛŒ
    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')
    
    # ØªØ¨Ø¯ÛŒÙ„ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¹Ø±Ø¨ÛŒ
    text = ''.join(ARABIC_TO_FA.get(ch, ch) for ch in text)
    
    # Ø­Ø°Ù Ø­Ø±Ú©Ø§Øª
    text = DIAC_RE.sub("", text)
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
    text = text.translate(DIGIT_MAP)
    
    # Ø­Ø°Ù ØªØ·ÙˆÛŒÙ„
    text = text.replace("\u0640", "")
    
    # ØªØ¨Ø¯ÛŒÙ„ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ø¨Ù‡ ÙØ§ØµÙ„Ù‡
    text = text.replace("\u200c", " ")
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§
    text = re.sub(r"\s+", " ", text).strip()
    
    # ØªØ¨Ø¯ÛŒÙ„ Ú¯ÛŒÙˆÙ…Ù‡ ÙØ§Ø±Ø³ÛŒ
    text = re.sub(r"[Â«Â»]", "\"", text)
    
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ (ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒØŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ùˆ Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ)
    text = re.sub(r'[^\u0600-\u06FF\u0020-\u007E\s]', '', text)
    
    return text.strip()

def process_audio(audio, sr):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØªÛŒ"""
    # Ø­Ø°Ù Ø³Ú©ÙˆØª
    audio, _ = librosa.effects.trim(audio, top_db=20)
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
    peak = np.abs(audio).max()
    if peak > 0:
        audio = (0.95 / peak) * audio
    
    # Ø­Ø°Ù DC offset
    audio = audio - np.mean(audio)
    
    return audio

def process_single_item(args):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡"""
    idx, item, sr_target = args
    
    try:
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†
        text = normalize_persian_text(item["sentence"])
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø·ÙˆÙ„ Ù…ØªÙ†
        if len(text) < 5 or len(text) > 500:
            return None
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµØ¯Ø§
        audio = item["audio"]["array"]
        audio = process_audio(audio, item["audio"]["sampling_rate"])
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Øª Ø²Ù…Ø§Ù†
        duration = len(audio) / sr_target
        if duration < MIN_DUR or duration > MAX_DUR:
            return None
        
        # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„
        filename = f"sample_{idx:06d}.wav"
        filepath = os.path.join(OUT_WAV_DIR, filename)
        sf.write(filepath, audio, sr_target, subtype="PCM_16")
        
        return (filepath, text, duration)
    
    except Exception as e:
        print(f"Error processing item {idx}: {e}")
        return None

def main():
    print("ğŸ”„ Loading dataset from HuggingFace...")
    ds = load_dataset(HF_DATASET, split="train", use_auth_token=True)
    ds = ds.cast_column("audio", Audio(sampling_rate=SR))
    
    print(f"ğŸ“Š Total samples in dataset: {len(ds)}")
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ
    print("ğŸµ Processing audio samples...")
    with ProcessPoolExecutor(max_workers=8) as executor:
        args = [(idx, item, SR) for idx, item in enumerate(ds)]
        results = list(tqdm(
            executor.map(process_single_item, args),
            total=len(ds),
            desc="Processing"
        ))
    
    # ÙÛŒÙ„ØªØ± Ù†ØªØ§ÛŒØ¬
    samples = [r for r in results if r is not None]
    
    # Ø¢Ù…Ø§Ø±
    total_duration = sum(s[2] for s in samples)
    print(f"\nğŸ“ˆ Statistics:")
    print(f"  - Valid samples: {len(samples)}/{len(ds)}")
    print(f"  - Total duration: {total_duration/3600:.2f} hours")
    print(f"  - Average duration: {total_duration/len(samples):.2f} seconds")
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (95% train, 5% validation)
    random.seed(42)
    random.shuffle(samples)
    
    n_val = max(300, int(0.05 * len(samples)))
    val_samples = samples[:n_val]
    train_samples = samples[n_val:]
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„â€ŒÙ„ÛŒØ³Øªâ€ŒÙ‡Ø§
    print("\nğŸ“ Writing filelists...")
    
    with open(os.path.join(OUT_FL_DIR, "train.txt"), "w", encoding="utf-8") as f:
        for filepath, text, _ in train_samples:
            f.write(f"{filepath}|{text}\n")
    
    with open(os.path.join(OUT_FL_DIR, "val.txt"), "w", encoding="utf-8") as f:
        for filepath, text, _ in val_samples:
            f.write(f"{filepath}|{text}\n")
    
    print(f"âœ… Dataset preparation complete!")
    print(f"  - Train samples: {len(train_samples)}")
    print(f"  - Validation samples: {len(val_samples)}")

if __name__ == "__main__":
    main()
EOF

# Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª
python prepare_dataset.py
```

### 4.2 - ØªÙˆÙ„ÛŒØ¯ ÙÙˆÙ†Ù… (Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø§Ù…Ø§ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)

```bash
cat > generate_phonemes.py << 'EOF'
import os
from phonemizer import phonemize
from phonemizer.backend import EspeakBackend
from phonemizer.separator import Separator
from tqdm import tqdm

def create_phoneme_filelists():
    """ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ ÙÙˆÙ†Ù…"""
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª phonemizer
    backend = EspeakBackend('fa', preserve_punctuation=True)
    separator = Separator(phone=' ', word='| ', syllable='')
    
    for split in ['train', 'val']:
        input_file = f"filelists/{split}.txt"
        output_file = f"filelists/{split}_phoneme.txt"
        
        print(f"Processing {split} split...")
        
        with open(input_file, 'r', encoding='utf-8') as inf:
            lines = inf.readlines()
        
        with open(output_file, 'w', encoding='utf-8') as outf:
            for line in tqdm(lines, desc=f"Phonemizing {split}"):
                path, text = line.strip().split('|')
                
                try:
                    phones = phonemize(
                        text,
                        language='fa',
                        backend='espeak',
                        separator=separator,
                        strip=True,
                        preserve_punctuation=True
                    )
                    phones = ' '.join(phones.split())  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§
                    outf.write(f"{path}|{phones}\n")
                except Exception as e:
                    print(f"Error phonemizing: {text[:50]}... - {e}")
                    outf.write(f"{path}|{text}\n")  # fallback to original
        
        print(f"âœ… Created {output_file}")

if __name__ == "__main__":
    create_phoneme_filelists()
EOF

python generate_phonemes.py
```

---

## âš™ï¸ **Ù…Ø±Ø­Ù„Ù‡ 5: Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„**

```bash
cat > configs/vits2_persian.json << 'EOF'
{
  "train": {
    "log_interval": 100,
    "eval_interval": 1000,
    "seed": 1234,
    "epochs": 2000,
    "learning_rate": 0.0002,
    "betas": [0.8, 0.99],
    "eps": 1e-09,
    "batch_size": 32,
    "fp16_run": true,
    "lr_decay": 0.999875,
    "segment_size": 8192,
    "init_lr_ratio": 1,
    "warmup_epochs": 0,
    "c_mel": 45,
    "c_kl": 1.0,
    "grad_clip_thresh": 5.0,
    "num_workers": 8,
    "checkpoint_interval": 5000,
    "use_sr_rates": false
  },
  "data": {
    "training_files": "filelists/train_phoneme.txt",
    "validation_files": "filelists/val_phoneme.txt",
    "text_cleaners": [],
    "max_wav_value": 32768.0,
    "sampling_rate": 22050,
    "filter_length": 1024,
    "hop_length": 256,
    "win_length": 1024,
    "n_mel_channels": 80,
    "mel_fmin": 0.0,
    "mel_fmax": null,
    "add_blank": true,
    "n_speakers": 0,
    "cleaned_text": true
  },
  "model": {
    "use_mel_posterior_encoder": true,
    "use_transformer_flows": true,
    "transformer_flow_type": "fft",
    "use_spk_conditioned_encoder": false,
    "use_noise_scaled_mas": true,
    "use_duration_discriminator": true,
    "ms_istft_vits": false,
    "mb_istft_vits": false,
    "istft_vits": false,
    "subbands": 4,
    "gen_istft_n_fft": 16,
    "gen_istft_hop_size": 4,
    "inter_channels": 192,
    "hidden_channels": 192,
    "filter_channels": 768,
    "n_heads": 2,
    "n_layers": 6,
    "kernel_size": 3,
    "p_dropout": 0.1,
    "resblock": "1",
    "resblock_kernel_sizes": [3, 7, 11],
    "resblock_dilation_sizes": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],
    "upsample_rates": [8, 8, 2, 2],
    "upsample_initial_channel": 512,
    "upsample_kernel_sizes": [16, 16, 4, 4],
    "n_layers_q": 3,
    "use_spectral_norm": false,
    "use_sdp": true
  }
}
EOF
```

---

## ğŸ¯ **Ù…Ø±Ø­Ù„Ù‡ 6: Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´**

### 6.1 - Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¢Ù…ÙˆØ²Ø´

```bash
cat > train_model.sh << 'EOF'
#!/bin/bash

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø­ÛŒØ·ÛŒ
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export CUDA_LAUNCH_BLOCKING=0

# Ù†Ø§Ù… Ù…Ø¯Ù„
MODEL_NAME="persian_tts_vits2"
CONFIG="configs/vits2_persian.json"

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ù„Ø§Ú¯
mkdir -p logs/$MODEL_NAME

# Ø´Ø±ÙˆØ¹ TensorBoard Ø¯Ø± background
echo "Starting TensorBoard..."
tensorboard --logdir=logs --port=6006 --bind_all &
TB_PID=$!
echo "TensorBoard PID: $TB_PID"

# Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´
echo "Starting training..."
python train.py \
    -c $CONFIG \
    -m $MODEL_NAME \
    2>&1 | tee logs/$MODEL_NAME/training.log

echo "Training completed!"
EOF

chmod +x train_model.sh
```

### 6.2 - Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø± tmux

```bash
# Ø§ÛŒØ¬Ø§Ø¯ session Ø¬Ø¯ÛŒØ¯
tmux new -s vits2_training

# Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´
./train_model.sh

# Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ø§Ø² tmux: Ctrl+B then D
# Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ú¯Ø´Øª: tmux attach -t vits2_training
```

---

## ğŸ“Š **Ù…Ø±Ø­Ù„Ù‡ 7: Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯**

### 7.1 - Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ real-time

```bash
cat > monitor_training.py << 'EOF'
import os
import time
import json
import matplotlib.pyplot as plt
from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
import numpy as np

def monitor_training(log_dir, model_name):
    """Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù¾ÛŒØ´Ø±ÙØª Ø¢Ù…ÙˆØ²Ø´"""
    
    while True:
        try:
            # Ø®ÙˆØ§Ù†Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† checkpoint
            ckpt_dir = f"logs/{model_name}"
            checkpoints = [f for f in os.listdir(ckpt_dir) if f.startswith("G_") and f.endswith(".pth")]
            
            if checkpoints:
                latest_ckpt = max(checkpoints, key=lambda x: int(x.split("_")[1].split(".")[0]))
                step = int(latest_ckpt.split("_")[1].split(".")[0])
                
                print(f"\n{'='*50}")
                print(f"Latest Checkpoint: {latest_ckpt}")
                print(f"Current Step: {step:,}")
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù† ØªØ®Ù…ÛŒÙ†ÛŒ
                estimated_total_steps = 200000  # ØªØ®Ù…ÛŒÙ†
                progress = (step / estimated_total_steps) * 100
                print(f"Progress: {progress:.2f}%")
                
                # Ø®ÙˆØ§Ù†Ø¯Ù† loss Ø§Ø² TensorBoard logs
                event_acc = EventAccumulator(log_dir)
                event_acc.Reload()
                
                if 'loss/g/total' in event_acc.Tags()['scalars']:
                    g_loss = event_acc.Scalars('loss/g/total')
                    if g_loss:
                        latest_g_loss = g_loss[-1].value
                        print(f"Generator Loss: {latest_g_loss:.4f}")
                
                if 'loss/d/total' in event_acc.Tags()['scalars']:
                    d_loss = event_acc.Scalars('loss/d/total')
                    if d_loss:
                        latest_d_loss = d_loss[-1].value
                        print(f"Discriminator Loss: {latest_d_loss:.4f}")
                
                print(f"{'='*50}")
                
        except Exception as e:
            print(f"Monitoring error: {e}")
        
        time.sleep(60)  # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù‡Ø± 60 Ø«Ø§Ù†ÛŒÙ‡

if __name__ == "__main__":
    monitor_training("logs/persian_tts_vits2", "persian_tts_vits2")
EOF

# Ø§Ø¬Ø±Ø§ Ø¯Ø± Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡
python monitor_training.py &
```

### 7.2 - Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ TensorBoard

```bash
# Ø§Ú¯Ø± Ø±ÙˆÛŒ Ø³Ø±ÙˆØ± remote Ù‡Ø³ØªÛŒØ¯:
# Ø¯Ø± Ù„ÙˆÚ©Ø§Ù„:
ssh -L 6006:localhost:6006 user@server_ip

# Ø³Ù¾Ø³ Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø±:
# http://localhost:6006
```

---

## ğŸ§ª **Ù…Ø±Ø­Ù„Ù‡ 8: ØªØ³Øª Ù…Ø¯Ù„**

### 8.1 - Ø§Ø³Ú©Ø±ÛŒÙ¾Øª inference

```bash
cat > inference.py << 'EOF'
import torch
import commons
import utils
from models import SynthesizerTrn
from text import text_to_sequence
import soundfile as sf
import os
import json

def load_model(checkpoint_path, config_path):
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„"""
    
    with open(config_path, "r") as f:
        hps = json.load(f)
    hps = utils.HParams(**hps)
    
    net_g = SynthesizerTrn(
        len(hps.symbols),
        hps.data.filter_length // 2 + 1,
        hps.train.segment_size // hps.data.hop_length,
        **hps.model
    ).cuda()
    
    _ = net_g.eval()
    _ = utils.load_checkpoint(checkpoint_path, net_g, None)
    
    return net_g, hps

def synthesize(text, model, hps, output_path="output.wav"):
    """ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§ Ø§Ø² Ù…ØªÙ†"""
    
    # ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ sequence
    text_norm = text_to_sequence(text, hps.data.text_cleaners)
    
    if hps.data.add_blank:
        text_norm = commons.intersperse(text_norm, 0)
    
    text_norm = torch.LongTensor(text_norm)
    text_len = torch.LongTensor([text_norm.size(0)])
    
    with torch.no_grad():
        x = text_norm.cuda().unsqueeze(0)
        x_len = text_len.cuda()
        
        # ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§
        audio = model.infer(
            x, 
            x_len, 
            noise_scale=0.667,
            noise_scale_w=0.8,
            length_scale=1.0
        )[0][0, 0].data.cpu().float().numpy()
    
    # Ø°Ø®ÛŒØ±Ù‡
    sf.write(output_path, audio, hps.data.sampling_rate)
    print(f"âœ… Audio saved to: {output_path}")
    
    return audio

def test_model():
    """ØªØ³Øª Ù…Ø¯Ù„ Ø¨Ø§ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
    
    # Ù…Ø³ÛŒØ±Ù‡Ø§
    checkpoint = "logs/persian_tts_vits2/G_100000.pth"  # Ø¢Ø®Ø±ÛŒÙ† checkpoint
    config = "configs/vits2_persian.json"
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
    print("Loading model...")
    model, hps = load_model(checkpoint, config)
    
    # Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªØ³Øª
    test_texts = [
        "Ø³Ù„Ø§Ù…ØŒ Ø§ÛŒÙ† ÛŒÚ© ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø± ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª.",
        "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø± Ø­Ø§Ù„ ØªØºÛŒÛŒØ± Ø¯Ù†ÛŒØ§ÛŒ Ù…Ø§ Ø§Ø³Øª.",
        "Ø§Ù…Ø±ÙˆØ² Ù‡ÙˆØ§ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ¨Ø§ Ùˆ Ø¢ÙØªØ§Ø¨ÛŒ Ø§Ø³Øª.",
        "Ú©ØªØ§Ø¨ Ø®ÙˆØ§Ù†Ø¯Ù† ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ù‡ØªØ±ÛŒÙ† Ø³Ø±Ú¯Ø±Ù…ÛŒâ€ŒÙ‡Ø§ Ø§Ø³Øª.",
        "ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ø±ÙˆØ² Ø¨Ù‡ Ø±ÙˆØ² Ù¾ÛŒØ´Ø±ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯."
    ]
    
    # ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§
    os.makedirs("test_outputs", exist_ok=True)
    
    for i, text in enumerate(test_texts):
        print(f"\nGenerating sample {i+1}...")
        print(f"Text: {text}")
        
        output_path = f"test_outputs/sample_{i+1}.wav"
        synthesize(text, model, hps, output_path)

if __name__ == "__main__":
    test_model()
EOF
```

### 8.2 - Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª

```bash
# Ø¨Ø¹Ø¯ Ø§Ø² Ø­Ø¯Ø§Ù‚Ù„ 50000 step
python inference.py
```

---

## ğŸ‰ **Ù…Ø±Ø­Ù„Ù‡ 9: Deployment**

### 9.1 - Ø§ÛŒØ¬Ø§Ø¯ Web API

```bash
cat > app.py << 'EOF'
from flask import Flask, request, send_file, jsonify
import torch
import io
import base64
from inference import load_model, synthesize
import soundfile as sf
import numpy as np

app = Flask(__name__)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ (ÛŒÚ©Ø¨Ø§Ø±)
print("Loading model...")
model, hps = load_model(
    "logs/persian_tts_vits2/G_best.pth",
    "configs/vits2_persian.json"
)
print("Model loaded!")

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "healthy"})

@app.route('/synthesize', methods=['POST'])
def synthesize_api():
    try:
        data = request.json
        text = data.get('text', '')
        
        if not text:
            return jsonify({"error": "No text provided"}), 400
        
        # ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§
        audio = synthesize(text, model, hps)
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ base64
        buffer = io.BytesIO()
        sf.write(buffer, audio, hps.data.sampling_rate, format='WAV')
        buffer.seek(0)
        audio_base64 = base64.b64encode(buffer.read()).decode('utf-8')
        
        return jsonify({
            "audio": audio_base64,
            "sample_rate": hps.data.sampling_rate
        })
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
EOF

# Ù†ØµØ¨ Flask
pip install flask

# Ø§Ø¬Ø±Ø§ÛŒ Ø³Ø±ÙˆØ±
python app.py
```

### 9.2 - Ø§ÛŒØ¬Ø§Ø¯ Gradio Interface

```bash
cat > gradio_app.py << 'EOF'
import gradio as gr
import torch
from inference import load_model, synthesize
import numpy as np

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
print("Loading model...")
model, hps = load_model(
    "logs/persian_tts_vits2/G_best.pth",
    "configs/vits2_persian.json"
)

def tts_persian(text, speed=1.0, noise_scale=0.667):
    """ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø±"""
    
    if not text:
        return None
    
    # ØªÙˆÙ„ÛŒØ¯ ØµØ¯Ø§
    audio = synthesize(
        text, 
        model, 
        hps,
        length_scale=1/speed,
        noise_scale=noise_scale
    )
    
    return (hps.data.sampling_rate, audio)

# Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ
iface = gr.Interface(
    fn=tts_persian,
    inputs=[
        gr.Textbox(
            label="Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ",
            placeholder="Ù…ØªÙ† Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯...",
            lines=3
        ),
        gr.Slider(
            minimum=0.5,
            maximum=2.0,
            value=1.0,
            step=0.1,
            label="Ø³Ø±Ø¹Øª Ú¯ÙØªØ§Ø±"
        ),
        gr.Slider(
            minimum=0.1,
            maximum=1.0,
            value=0.667,
            step=0.01,
            label="ØªÙ†ÙˆØ¹ ØµØ¯Ø§"
        )
    ],
    outputs=gr.Audio(label="ØµØ¯Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡"),
    title="ğŸ™ï¸ Persian Text-to-Speech (VITS2)",
    description="Ø³ÛŒØ³ØªÙ… ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø± ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² VITS2",
    examples=[
        ["Ø³Ù„Ø§Ù…ØŒ Ø¨Ù‡ Ø³ÛŒØ³ØªÙ… ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ú¯ÙØªØ§Ø± ÙØ§Ø±Ø³ÛŒ Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯.", 
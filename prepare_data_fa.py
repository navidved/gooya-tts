import os
import re
import random
import unicodedata
import soundfile as sf
import librosa
import numpy as np
from datasets import load_dataset
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
HF_DATASET = "navidved/approved-tts-dataset"
OUT_WAV_DIR = "data_fa/wavs"
OUT_FL_DIR = "filelists"
SR = 22050
MIN_DUR = 1.0   # Ø­Ø¯Ø§Ù‚Ù„ 1 Ø«Ø§Ù†ÛŒÙ‡
MAX_DUR = 15.0  # Ø­Ø¯Ø§Ú©Ø«Ø± 15 Ø«Ø§Ù†ÛŒÙ‡

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒâ€ŒÙ‡Ø§
os.makedirs(OUT_WAV_DIR, exist_ok=True)
os.makedirs(OUT_FL_DIR, exist_ok=True)

# Ù†Ú¯Ø§Ø´Øª Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
ARABIC_TO_FA = {
    '\u064A': 'ÛŒ', '\u0643': 'Ú©', '\u06C0': 'Ù‡', 
    '\u06CC': 'ÛŒ', '\u0649': 'ÛŒ', '\u06A9': 'Ú©'
}

# ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
PERSIAN_DIGITS = "Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹"
LATIN_DIGITS = "0123456789"
DIGIT_MAP = {ord(p): LATIN_DIGITS[i] for i, p in enumerate(PERSIAN_DIGITS)}

# Ø­Ø±Ú©Ø§Øª Ø¹Ø±Ø¨ÛŒ
DIACRITICS = ''.join(chr(c) for c in range(0x064B, 0x065F+1)) + "\u0670"
DIAC_RE = re.compile(f"[{re.escape(DIACRITICS)}]")

def normalize_persian_text(text):
    """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ú©Ù†ØªØ±Ù„ÛŒ
    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')
    
    # ØªØ¨Ø¯ÛŒÙ„ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¹Ø±Ø¨ÛŒ
    text = ''.join(ARABIC_TO_FA.get(ch, ch) for ch in text)
    
    # Ø­Ø°Ù Ø­Ø±Ú©Ø§Øª
    text = DIAC_RE.sub("", text)
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
    text = text.translate(DIGIT_MAP)
    
    # Ø­Ø°Ù ØªØ·ÙˆÛŒÙ„
    text = text.replace("\u0640", "")
    
    # ØªØ¨Ø¯ÛŒÙ„ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ø¨Ù‡ ÙØ§ØµÙ„Ù‡
    text = text.replace("\u200c", " ")
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§
    text = re.sub(r"\s+", " ", text).strip()
    
    # ØªØ¨Ø¯ÛŒÙ„ Ú¯ÛŒÙˆÙ…Ù‡ ÙØ§Ø±Ø³ÛŒ
    text = re.sub(r"[Â«Â»]", "\"", text)
    
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ
    text = re.sub(r'[^\u0600-\u06FF\u0020-\u007E\s]', '', text)
    
    return text.strip()

def process_audio(audio_path, target_sr=22050):
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØªÛŒ"""
    try:
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ librosa
        audio, sr = librosa.load(audio_path, sr=target_sr, mono=True)
        
        # Ø­Ø°Ù Ø³Ú©ÙˆØª
        audio, _ = librosa.effects.trim(audio, top_db=20)
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        peak = np.abs(audio).max()
        if peak > 0:
            audio = (0.95 / peak) * audio
        
        # Ø­Ø°Ù DC offset
        audio = audio - np.mean(audio)
        
        return audio, target_sr
    except Exception as e:
        print(f"Error loading audio {audio_path}: {e}")
        return None, None

def main():
    print("ğŸ”„ Loading dataset from HuggingFace...")
    
    # Ù„ÙˆØ¯ Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø¯ÙˆÙ† Audio casting
    ds = load_dataset(HF_DATASET, split="train")
    
    print(f"ğŸ“Š Total samples in dataset: {len(ds)}")
    
    samples = []
    skipped = 0
    
    print("ğŸµ Processing audio samples...")
    for idx, item in enumerate(tqdm(ds, desc="Processing")):
        try:
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†
            text = normalize_persian_text(item["sentence"])
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø·ÙˆÙ„ Ù…ØªÙ†
            if len(text) < 5 or len(text) > 500:
                skipped += 1
                continue
            
            # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… audio field Ø­Ø§ÙˆÛŒ path ÛŒØ§ bytes Ø§Ø³Øª
            audio_data = item["audio"]
            
            # Ø§Ú¯Ø± audio ÛŒÚ© dictionary Ø§Ø³Øª Ø¨Ø§ array Ùˆ sampling_rate
            if isinstance(audio_data, dict) and 'array' in audio_data:
                audio_array = np.array(audio_data['array'])
                original_sr = audio_data.get('sampling_rate', SR)
                
                # resample Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø§Ø³Øª
                if original_sr != SR:
                    audio_array = librosa.resample(audio_array, orig_sr=original_sr, target_sr=SR)
                
            # Ø§Ú¯Ø± audio ÛŒÚ© path Ø§Ø³Øª
            elif isinstance(audio_data, str):
                audio_array, _ = process_audio(audio_data, SR)
                if audio_array is None:
                    skipped += 1
                    continue
            
            # Ø§Ú¯Ø± audio ÛŒÚ© bytes object Ø§Ø³Øª
            elif isinstance(audio_data, bytes):
                # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙ‚Øª Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
                temp_path = f"/tmp/temp_audio_{idx}.wav"
                with open(temp_path, 'wb') as f:
                    f.write(audio_data)
                audio_array, _ = process_audio(temp_path, SR)
                os.remove(temp_path)
                if audio_array is None:
                    skipped += 1
                    continue
            else:
                # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…Ø³ØªÙ‚ÛŒÙ…
                try:
                    audio_array = np.array(audio_data)
                except:
                    print(f"Unknown audio format for item {idx}")
                    skipped += 1
                    continue
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµØ¯Ø§
            # Ø­Ø°Ù Ø³Ú©ÙˆØª
            audio_array, _ = librosa.effects.trim(audio_array, top_db=20)
            
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
            peak = np.abs(audio_array).max()
            if peak > 0:
                audio_array = (0.95 / peak) * audio_array
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Øª Ø²Ù…Ø§Ù†
            duration = len(audio_array) / SR
            if duration < MIN_DUR or duration > MAX_DUR:
                skipped += 1
                continue
            
            # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ WAV
            filename = f"sample_{idx:06d}.wav"
            filepath = os.path.join(OUT_WAV_DIR, filename)
            sf.write(filepath, audio_array, SR, subtype="PCM_16")
            
            samples.append((filepath, text, duration))
            
        except Exception as e:
            print(f"Error processing item {idx}: {e}")
            skipped += 1
            continue
    
    print(f"\nğŸ“Š Processing complete:")
    print(f"  - Valid samples: {len(samples)}")
    print(f"  - Skipped samples: {skipped}")
    
    if len(samples) == 0:
        print("âŒ No valid samples found!")
        return
    
    # Ø¢Ù…Ø§Ø±
    total_duration = sum(s[2] for s in samples)
    print(f"  - Total duration: {total_duration/3600:.2f} hours")
    print(f"  - Average duration: {total_duration/len(samples):.2f} seconds")
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (95% train, 5% validation)
    random.seed(42)
    random.shuffle(samples)
    
    n_val = max(300, int(0.05 * len(samples)))
    val_samples = samples[:n_val]
    train_samples = samples[n_val:]
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„â€ŒÙ„ÛŒØ³Øªâ€ŒÙ‡Ø§
    print("\nğŸ“ Writing filelists...")
    
    with open(os.path.join(OUT_FL_DIR, "train.txt"), "w", encoding="utf-8") as f:
        for filepath, text, _ in train_samples:
            f.write(f"{filepath}|{text}\n")
    
    with open(os.path.join(OUT_FL_DIR, "val.txt"), "w", encoding="utf-8") as f:
        for filepath, text, _ in val_samples:
            f.write(f"{filepath}|{text}\n")
    
    print(f"âœ… Dataset preparation complete!")
    print(f"  - Train samples: {len(train_samples)}")
    print(f"  - Validation samples: {len(val_samples)}")

if __name__ == "__main__":
    main()